{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as pylab\nimport seaborn as sns\n\nimport re\nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, GlobalAveragePooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n#rom keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint \nfrom keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n\ntest_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")\ntest_data.columns = ['id','comment_text','lang']\nvalidation_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain_data.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include='all')","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                      id                             comment_text  \\\ncount             223549                                   223549   \nunique            223549                                   223549   \ntop     b77ce4ce676c34ac  I love Melina sooo much! MNM is awesome   \nfreq                   1                                        1   \nmean                 NaN                                      NaN   \nstd                  NaN                                      NaN   \nmin                  NaN                                      NaN   \n25%                  NaN                                      NaN   \n50%                  NaN                                      NaN   \n75%                  NaN                                      NaN   \nmax                  NaN                                      NaN   \n\n                toxic  \ncount   223549.000000  \nunique            NaN  \ntop               NaN  \nfreq              NaN  \nmean         0.095657  \nstd          0.294121  \nmin          0.000000  \n25%          0.000000  \n50%          0.000000  \n75%          0.000000  \nmax          1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>223549</td>\n      <td>223549</td>\n      <td>223549.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>223549</td>\n      <td>223549</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>b77ce4ce676c34ac</td>\n      <td>I love Melina sooo much! MNM is awesome</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.095657</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.294121</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(train_data).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in [train_data, test_data]:\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ll', ' will'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ve', ' have'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('don\\'t', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('dont', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('aren\\'t', ' are not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('won\\'t', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('wont', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('can\\'t', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('cant', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shan\\'t', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shant', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'m', ' am'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesn't\", \"does not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesnt\", \"does not\"))                                                      \n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasn't\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasnt\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"haven't\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"havent\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"it's\" , \"it is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"that's\" , \"that is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"weren't\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"werent\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' u ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' U ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: re.sub('[\\(\\)\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890â€”]', ' ', x).strip())","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nplt.figure(figsize=(7,7))\nplt.title('Correlation of Features & Targets',y=1.05,size=13)\nsns.heatmap(train_data[target_columns].astype(float).corr(),linewidths=0.2,vmax=1.0,square=True,annot=True)\nplt.show()\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y = train_data[target_columns]\nY = train_data['toxic']\nY","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n223544    0\n223545    0\n223546    0\n223547    1\n223548    0\nName: toxic, Length: 223549, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 20000\nmax_length = 100\nembed_size = 300\nbatch_size = 1024\nepochs = 2","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTokenization\n\"\"\"\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_data['comment_text'])\n\ntrain_tokenized = tokenizer.texts_to_sequences(train_data['comment_text'])\ntest_tokenized = tokenizer.texts_to_sequences(test_data['comment_text'])\n\nX = pad_sequences(train_tokenized, maxlen=max_length)\nX_ = pad_sequences(test_tokenized, maxlen=max_length)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nEmbedding Matrix\n\"\"\"\nembedding_index = {}\nwith open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        embedding_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\nword_index = tokenizer.word_index\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(max_length,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", activation=\"relu\")(x)\n\nx = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPool1D()(x)])\n\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\nmodel.summary()","execution_count":24,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 100)]        0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 100, 300)     6000000     input_2[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 100, 256)     330240      embedding_1[0][0]                \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 98, 64)       49216       bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n                                                                 global_max_pooling1d_1[0][0]     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n==================================================================================================\nTotal params: 6,379,585\nTrainable params: 6,379,585\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"0         0\n1         0\n2         0\n3         0\n4         0\n         ..\n223544    0\n223545    0\n223546    0\n223547    1\n223548    0\nName: toxic, Length: 223549, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\nmodel.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n              callbacks=[reduce_lr])\n","execution_count":26,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n197/197 [==============================] - 981s 5s/step - loss: 0.1407 - accuracy: 0.9454 - val_loss: 0.1423 - val_accuracy: 0.9362 - lr: 0.0010\nEpoch 2/2\n197/197 [==============================] - 984s 5s/step - loss: 0.1004 - accuracy: 0.9604 - val_loss: 0.1469 - val_accuracy: 0.9348 - lr: 0.0010\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f464900d750>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sumbission_file = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n#sumbission_file = sumbission_file.drop('toxic',axis=1)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = model.predict(X_)\n#cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\"\"\"\nfor i in cols:\n    sumbission_file[i]=\"\"\n\"\"\"\n\n\nsumbission_file['toxic'] = sub\nsumbission_file.to_csv('submission.csv', index=False)","execution_count":28,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}