{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, GlobalAveragePooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "#rom keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint \n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")\n",
    "test_data.columns = ['id','comment_text','lang']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_data.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>223549</td>\n",
       "      <td>223549</td>\n",
       "      <td>223549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>223549</td>\n",
       "      <td>223549</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>b77ce4ce676c34ac</td>\n",
       "      <td>I love Melina sooo much! MNM is awesome</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                             comment_text  \\\n",
       "count             223549                                   223549   \n",
       "unique            223549                                   223549   \n",
       "top     b77ce4ce676c34ac  I love Melina sooo much! MNM is awesome   \n",
       "freq                   1                                        1   \n",
       "mean                 NaN                                      NaN   \n",
       "std                  NaN                                      NaN   \n",
       "min                  NaN                                      NaN   \n",
       "25%                  NaN                                      NaN   \n",
       "50%                  NaN                                      NaN   \n",
       "75%                  NaN                                      NaN   \n",
       "max                  NaN                                      NaN   \n",
       "\n",
       "                toxic  \n",
       "count   223549.000000  \n",
       "unique            NaN  \n",
       "top               NaN  \n",
       "freq              NaN  \n",
       "mean         0.095657  \n",
       "std          0.294121  \n",
       "min          0.000000  \n",
       "25%          0.000000  \n",
       "50%          0.000000  \n",
       "75%          0.000000  \n",
       "max          1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.isnull(train_data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_data, test_data]:\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ll', ' will'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ve', ' have'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('don\\'t', ' do not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('dont', ' do not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('aren\\'t', ' are not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('won\\'t', ' will not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('wont', ' will not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('can\\'t', ' cannot'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('cant', ' cannot'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shan\\'t', ' shall not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shant', ' shall not'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'m', ' am'))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesn't\", \"does not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesnt\", \"does not\"))                                                      \n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasn't\", \"has not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasnt\", \"has not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"haven't\", \"have not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"havent\", \"have not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"it's\" , \"it is\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"that's\" , \"that is\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"weren't\" , \"were not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"werent\" , \"were not\"))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' u ', ' you '))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' U ', ' you '))\n",
    "    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: re.sub('[\\(\\)\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890—]', ' ', x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Correlation of Features & Targets',y=1.05,size=13)\n",
    "sns.heatmap(train_data[target_columns].astype(float).corr(),linewidths=0.2,vmax=1.0,square=True,annot=True)\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "223544    0\n",
       "223545    0\n",
       "223546    0\n",
       "223547    1\n",
       "223548    0\n",
       "Name: toxic, Length: 223549, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y = train_data[target_columns]\n",
    "Y = train_data['toxic']\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "max_length = 100\n",
    "embed_size = 300\n",
    "batch_size = 1024\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenization\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_data['comment_text'])\n",
    "\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_data['comment_text'])\n",
    "test_tokenized = tokenizer.texts_to_sequences(test_data['comment_text'])\n",
    "\n",
    "X = pad_sequences(train_tokenized, maxlen=max_length)\n",
    "X_ = pad_sequences(test_tokenized, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding Matrix\n",
    "\"\"\"\n",
    "embedding_index = {}\n",
    "with open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        embedding_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     6000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 256)     330240      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 64)       49216       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,379,585\n",
      "Trainable params: 6,379,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_length,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", activation=\"relu\")(x)\n",
    "\n",
    "x = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPool1D()(x)])\n",
    "\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "223544    0\n",
       "223545    0\n",
       "223546    0\n",
       "223547    1\n",
       "223548    0\n",
       "Name: toxic, Length: 223549, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "197/197 [==============================] - 981s 5s/step - loss: 0.1407 - accuracy: 0.9454 - val_loss: 0.1423 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "197/197 [==============================] - 984s 5s/step - loss: 0.1004 - accuracy: 0.9604 - val_loss: 0.1469 - val_accuracy: 0.9348 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f464900d750>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
    "              callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumbission_file = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n",
    "#sumbission_file = sumbission_file.drop('toxic',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = model.predict(X_)\n",
    "#cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\"\"\"\n",
    "for i in cols:\n",
    "    sumbission_file[i]=\"\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sumbission_file['toxic'] = sub\n",
    "sumbission_file.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
